1. `pytorch` 中的 `lstm` 有没有 时间步 time step

   > ?
   
2. 如果纯粹一点，我的IRM实验应该怎么做

   > 512 256
   >
   > train 数据是 4620 中的 4000 * 2 = 8000 句，乘以不同的 snr 和 noise   
   >
   > test 数据是 162 中 选 50 个 然后 进行 
   >
   > 

3. `pytorch` 中新建张量的方法

   > `ipt = torch.randn(2,355,160)`

4. 一维卷积的输入shape

   > [N , C, Seq_len]

5. 自回归

   就是使用 $x_1,\cdots,x_{t-1}$ 来预测 $x_t$

6. 语音中使用tranformer的时候是将feedforward改成了一维卷积，这有什么道理？

   > 不是很明确有什么用，
   >
   > 1. 可能是有用的，就像fastspeech说的那样是为了
   >
   >    The motivation is that the adjacent hidden states are more closely related in the character/phoneme and mel-spectrogram sequence in speech tasks. 
   >
   > 2. 没有什么用

7. fastspeech 中使用了 非自回归 的形式，有什么好处？

   > 加快速度，

8. RT 中使用的是非回归形式吗？

   > 

9. IRM 中的model的输入是什么？是什么形状的？

   > 一条语音是 [ T , D] 即 [10, 257]
   >
   > 多条语音合在一起就是 [T, Batch_size ,  D] 即 [10 , 32, 257]
   >
   > ​		合一起的时候需要pad 一下，补齐 T,  然后就可以整体合并了
   >
   > 因为pytorch 中是 Batch first 所以是 [B, T, D] 即 [32, 10, 257]

10. Paddle 中 seq2seq 的输入是什么样子的？

   > 生成器 创建 一个 大的数据 
   >
   > librosa 返回的是一个 [ D , T ]，使用list存着，
   >
   > [  [T,D], [T,D],[T,D] ]
   >
   > 使用 prepare_input 来手动获取 batch 个,就是循环读取的样子

11. model 中每一层是变成什么样的？

    > 可以看看那个视频的说明

12. pytorch 中怎么样可以类型转换？

13. pytorch 中 cuda( ) 的作用是什么？

    >加了cuda( ) ，gpu版本是torch.cuda.FloatTensor
    >
    >cpu版本是torch.FloatTensor

14. ```shell
      arguments are located on different GPUs at /opt/conda/conda-bld/pytorch_1573049306803/work/aten/src/THC/generic/THCTensorMasked.cu:28  # 是什么原因？
    ```

15. 为什么求解偶问题？

    > 为了减少复杂度	

16. 什么是一维卷积？

    > 就是卷积时只看纵列

17. model 中 lstm 是怎么样计算 (B, T, D) 的？

    

18. mask 部分是怎么操作的？

19. RT 网络结构中的 Pooling 操作是什么？

    > 其实就是多头注意力机制

20. python 中生成器是什么？

   > 就是用于创建【迭代器】的简单而强大的工具。
   >
   > 对生成器调用next( ) ，它会从上次离开位置恢复执行。
   >
   > 也就是说，for 循环 会在上次的地方恢复然后执行下一个

11. python 中 访问 什么用的到 enumerate( ) ？

    > 序列用的是 enumerate
    >
    > 字典是用item

12. Python中循环的技巧有哪些？

    >  items( ) 来取 字典
    >
    > enumerate( ) 来取序列
    >
    > 多个序列使用 zip( )

13. 怎么把 mag 和 phase 合在一起？

   > mag * phase

11. tensorflow 的 dataloader 怎么写的？

12. python 中的 成员 可以有其他 对象？

13. `librosa.magphase` 返回的 shape 是什么样的？

    （d，t）比如 （257，10）

14. IRM 的公式是什么？

15. LPS 的公式是什么？

16. pesq是测试什么的？

17. stoi是测试什么的？

18. 多目标是怎么实现的？

19. stoi + pesq 的 loss 是怎么实现的？

20. 怎么实现多GPU训练？

    > 有好几种，有一种可以 每一层为一个 GPU
    >
    > torch.nn.linear(10, 5).to('cuda:1')

21. 论文怎么写呢~先写中文
   ```

   ```