开头第一句：

To evaluate the performance of the proposed method, we create some synthetic datasets。

The conditions of all the datasets are summarized in Table 1.

**版本**1：

Specifically, first we randomly select a clean speech file from a multiple-speaker speech corpus which has 21407 utterances (about 24.5 hours) and a noise file from the Musan corpus [29].

Then we randomly select an SNR between 0dB and 20dB, and mix these two files to create a noisy file according to the selected SNR. 

We divide the generated 21407 noisy files into 13407, 4000 and 3000 for training, validation and testing. 

Neural network training is conducted using the training set and the loss on the validation set is examined as the convergence condition.

The original testing set is named as Test-0. 

We further create 4 new test sets (Test-1,2,3,4) each with 3000 utterances from another set of speakers for real scenarios. In Test-1,2, interferer noises are from the noise pool, Musan, as Test-0. 

These Test-3,4 are generated by adding different noises (with the training set) from CHIME3 [30]. 

The SNR of these test sets are illustrated as Table 1.

**版本**2

The experiments were conducted on the TIMIT database [25]. 

We used 115 noise types in the training stage to improve the generalization capacity of unseen environments. 

All 4620 utterances from the TIMIT training set were corrupted with each noise type at six SNR levels, i.e., 20dB, 15dB, 10dB, 5dB, 0dB, and -5dB, to build 10-hour multi-condition training set, consisting of pairs of clean and noisy speech utterances. 

The 192 utterances from the core test set of TIMIT database were used to construct the test set for each combination of noise types and SNR levels. 

To evaluate on unseen noise types, three other noise types, namely Buccaneer1, Destroyer engine and HF channel from the NOISEX-92 corpus [26], were adopted. 

The quality and intelligibility of the enhanced speech were measured by perceptual evaluation of speech quality (PESQ) [27], and short-time objective intelligibility (STOI) [28], respectively.

As for the front-end, all signals were sampled at 16kHz rate. The frame length and shift were 256 and 128 samples, respectively. 

The 257-dimensional feature vector was used for both LPS and IRM targets. The computational network toolkit (CNTK) [29] was used for training.

The DNN model consisted of the 1799-dimensional input layer (7-frame ex- pansion), 3 hidden layers with 2048 nodes for each layer.

**版本**3





---



自己的版本：

To evaluate the performance of the proposed method, we create some synthetic datasets.

We summarizes the conditions for all datasets as Table 1 showing.

Specifically, first we randomly select a clean speech file from the TIMIT[ ] trainning set which has 4620 utterances  and a noise file from the Musan corpus [29].

Then we randomly select an SNR between 0dB and 20dB, and mix these two files to create a noisy file according to the selected SNR. 

We divide the generated 4600 noisy files into 900, 400 for training, validation

The 192 utterances from the core test set of TIMIT database were used to construct the test set for each combination of noise types and SNR levels.

/**

Testing sets with 400 utterances from another set of speakers  .

We further create a new test set (Test-1)  generated by adding different noises (with the training set) from CHIME3 [30].

**/

The SNR of these test sets are illustrated as Table 1.