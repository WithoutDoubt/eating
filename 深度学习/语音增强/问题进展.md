#### 问题背景

​	目前很多的语音增强算法仅仅改善了语音质量，在可懂度方面存在很大不足，即在低信噪比条件下，噪声得到了减少，但引入了较大的语音失真。因此，语音增强的主要挑战是如何设计一个高效的算法，在不引入语音失真的前提下，有效抑制噪声和干扰。

​	语音增强的具体解决方案与应用场景、噪声或者干扰信号的特性、噪声与语音的关系、麦克风的数量密切相关。干扰信号可能是类似噪声的信号( 如风扇噪声) ，也有可能是类似语音的信号( 如多人说话时其他说话人的声音) ; 噪声对于信号而言可能是加性的，也有可能是乘性的( 如房间混响情况) 。再者，噪声与纯净的语音信号之间可能是统计相关或者无关的; 麦克风可以使用单个，也可以使用多个。麦克风阵列可有效排除噪声干扰，达到减少语音失真 的目的，但其缺点是需要准确的声源定位信息。

​	由上可以看出，语音增强是一项较为复杂和繁琐的科研课题，要同时照顾到主、客观的听觉感受，这不仅涉及到信号处理方面的知识，同时还要基于人耳听觉感知和语音学的相关原理。

​	由于单麦克风语音增强不需要声源定位，同时对混响不敏感，因此，以谱减法、统计模型法和子空间法为代表的无监督学习方法，在过去的几十年里得到了深入研究，并衍生出众多的改良方法，在消除噪声方面发挥了重要作用。



#### 传统方法

经典的无监督学习的语音增强方法能在平稳噪声条件下取得令人满意的增强效果，但对非平稳噪声而言，这类方法的性能不佳，原因是他们不能适应噪声随时随地的变化。

- [ ] 谱减法［3-4］

  假设噪声是加性的，它能从观测信号谱中减去估计的噪声谱，得到重构的语音谱。

  

- [ ] 统计模型法［5-8］

  将语音增强作为统计估计框架中的一个问题提出，即在给定观测信号的测量参数( 如傅里叶变换系数) 后，希望对目标参数( 语音信号的傅里叶变换系数) 进行线性或非线性估计。

- [ ] 子空间算法［9-11］

  则以线性代数理论为基础，即将观测信号的向量空间分解为两个子空间，其中一个子空间主要包括纯净信号，另一个子空间主要包括噪声信号，这样就可以简单的通过清除观测信号向量空间中“噪声子空间”的部分内容，达到估计纯净信号的目的。

---

<font color=red>基于深度神经网络的有监督学习的语音增强算法充分利用了语音的先验信息，克服了无监督学习方法所存在的很多假设和噪声估计不准确问题，大大提升了增强性能。</font>

#### `MLP(multilayer perceptions)`

- 2013年 加权去噪自动编码器[14-15]：
  - [ ] 用于描述纯净语音和含噪语音功率谱的仿射关系，较好地估计了纯净语音的功率谱，提高的语音增强的性能。
- 2014年 含噪语音的对数功率 (log-power spectral) 作为`MLP`的输入[16]
  - [ ] 利用`MLP`  非线性函数匹配的能力，在`MLP` 的输出端预测得到纯净语音的`LPS`。
  - [ ] 首先对带噪语音 $y(n)$ 进行特征提取，得到带噪语音的幅度谱$Y_1(k,m)$及其相位角 $\theta$，
  - [ ] 然后利用之前已经训练好的`DNN`, 将带噪语音的特征`Y_l(k,m)`作为输入，即可获得所需的训练标$\hat{X}_l(k,m)$。
  - [ ] 通过波形重构，获得增强语音$\hat{x}(n)$
- 优点：该方法在进行语音增强的时候，没有对语音和噪声之间的关系做任何假设，因此该方法能够适用于广泛的噪声环境中且能取得更好的增强表现。
- 2015年[17]三种策略对原有网络进行改进
  - [ ] 解决神经网络过匹配的<font color=red>全局方差均衡策略</font>----------GV
  - [ ] 提升神经网络泛化能力的<font color=red>退出</font>策略 ------------------Dropout
  - [ ] 提高网络泛化能力的<font color=red>噪声意识训练</font>-------------------NAT
- 2015年多目标学习[23] -------同时学习多个纯净语音特征的方法
  - [ ] 将噪声的LPS作为网络输入，去同时匹配纯净语音的LPS、梅尔倒谱系数、理想二值掩膜 ( ideal binary mask，IBM)能实现更好的增强效果
- 2017年多目标学习的方法[24]
  - [ ] 将纯净语音的`LPS`及理想比值掩膜(ideal ratio mask, `IRM` )作为学习目标
  - [ ] 利用语音的<font color=red>子带特征</font>进行噪声感知训练
- D.L Wang 的研究[25 26]
  - [ ] 利用语音的<font color=red>频域特征</font>或根据<font color=red>计算听觉场景分析所得到的特征</font>作为输入
  - [ ] 预测 `IRM` 或者 `IBM`  
  - [ ] 虽然假定了语音和噪声是相互独立的，但是实验结果发现，性能并不差于利用 `MLP`直接估计`LPS`
- [27 28] 结合了相位信息
- [29 30] 传统方法和神经网络语音增强相结合

---

#### `CNNs (convolutional neural networks)`

<font color=red>与MLP相比，CNN能够更加准确地获取输入语音信号的局部特征</font>

在语音增强中，可以更好地恢复出语音信号的高频成分，提高语音增强语音的质量以及可懂度

- 2017年[32] 用一维CNN实现了语音增强

  - [ ] 卷积层 -> 最大池化层 -> 卷积层 -> 两个全连接层

  - [ ] `LPS`作为网络输入，预测 `IRM` 以及 `LPS`

  - [ ] `pesq` ：2.15       ->       2.19    【-3dB 时，MLP掩蔽   CNN掩蔽】

  - [ ] `stoi` ： 2.22      ->       2.36    【-3db时，MLP映射    CNN映射】

  - [ ] 证明了（1）具有拓扑结构的CNN比MLP结构的DNN更有效

    ​            （2）映射式增强比掩蔽式增强的效果好

  - [ ] <font color=red>缺点</font>：仍然含有全连接层，因此网络所含参数较多

- [33]使用全卷积层，中间使用残差连接[34]。

  - [ ] 参数是`MLP`的千分之一
  - [ ] 性能不差

- [35]运用全卷积神经网络直接匹配得到纯净语音时域波形

- [36]运用二维卷积神经网络，

  - [ ] 运用降噪自编码器的单声道声源分离算法

- [64]基于堆叠式的卷积自编码器语音增强

  - [ ] 语音对数谱作为输入
  - [ ] 自回归参数 作为 训练目标 ， 结合 码书谐波恢复算法 实现语音增强
  - [ ] 对带噪语音的谐波恢复取得了较好的效果

- [65]基于`MLP`的自编码器

  - [ ] 将带噪语音对数谱作为输入特征，预测一系列自回归参数，结合维纳滤波器实现语音增强
  - [ ] 语音谐波恢复效果不错

---

#### `RNNs (recurrent neural networks)`

以往利用`MLP`和`CNN`进行语音增强时，很难将某一帧语音和其周围帧的关系都考虑到，这限制了 `MLP` 和 `CNN` 在进行语音增强时的性能。

`RNN` 可以利用之前的信息对当前内容进行预测和判断。因此，更适用于处理和时间相关的信息。

训练 `RNN` 所用的方法是时序反向传播算法（Back Propagation Through Time, BPTT）

- [38] Andrew 等人提出使用 循环降噪自编码器
  - [ ] 含噪语音特征作为降噪自编码器网络的输入
  - [ ] 预测纯净语音的梅尔倒谱系数
  - [ ] 有效提高语音识别的准确率
- [39] 使用`RNN`进行唱歌信号的语音分离方法，
  - [ ] 从当前唱歌信号中分别分离出人声信号及纯净音乐信号、
  - [ ] `RNN` 同时预测人声信号 和 纯净音乐信号的幅度谱
  - [ ] 用该幅度谱去估计人声和音乐信号的掩膜值
  - [ ] 结合观测信号的相位信息得到所期望的信号。
  - [ ] 针对不同的音源分离任务，使用不同的结构
  - [ ] 存在梯度消失或者梯度爆炸的问题，严重影响了`RNN`的性能。所以，引入一个记忆细胞模块，称为`LSTM`
- `LSTM`【Long short-term memory】可以更好地获得噪声、带噪语音以及纯净语音内部间的时序相关信息，提高增强语音质量
- [43] 用`LSTM-RNN`分别预测纯净语音的对数谱和`IRM`来实现语音增强。
  - [ ] 也进行了多目标学习，用一个`LSTM-RNN`同时预测了纯净语音对数谱和`IRM`

`MLP、CNN、RNN、LSTM` 在实现语音增强上有各自的优势，但也有各自的不足。因此，这三种网络结构结合在一起，或许可以实现最佳的语音增强效果。

---

#### `GAN (generative adversarial networks)`

GAN 是2014年提出的一种强大的神经网络结构。GAN可以在给定随机噪声输入的情况下，生成出于真实信号相一致的东西。

GAN能够从一个随机分布的样本空间 $z$ 中，生成真实的样本数据 $x$。对于一个GAN，可以将其分为两个部分，一个生成器（G）,和一个判别器（D）。

生成器：利用一些随机样本去生成模仿真实分布的数据，进而去欺骗判别器。

判别器：区分给它的数据是生成器生成的，还是 确定就是真实的数据。

- 由于生成的真实数据样本存在一定的不确定性，所以提出了<font color=red>条件生成对抗网络</font>
  - [ ] 在 `cGAN` 中，它的生成器和判别器的输入多加入了一个观测得到的矢量 $y$。
  - [ ] 因此，生成器能够在 $y$ 的控制下合成所期望的数据。
- 2017 年[49]，`Pascual`将GAN运用到语音增强上
  - [ ] 含噪语音的时域波形 作为 G 的输入，期望 G 能够通过训练生成出纯净语音的时域波形
  - [ ] G 和 D 的网络结构都是一维的CNN，且加入了残差连接
  - [ ] 误差函数除了本身的误差函数，还加入了 文献[56]的`L1`误差
  - [ ] 缺点：没有说明 GAN 在语音增强上与以往的比较，谁的性能好，但是为今后研究提供了新的方向

---

#### `Transformer`

 

---