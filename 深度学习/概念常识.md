1. `hard attention` :  就是 0/1 问题，哪些区域是被 attention , 哪些不被关注 

   `soft attention` ：[0,1] 间连续分布问题，

   `self-attention` ：就是特征`feature map` 间自主学习，分配权重

2. attention 计算的时候为什么要除以 $\sqrt{d}$  ： d 是Q K 的维度，防止数据过大，`softmax`之后差异过小

3. attention可视化 怎么操作：数值 * 灰度值 255 , 再拼接为一个图片

4. 

